---
title: "52414 Lab 3: Text Wrangling, Sampling"
author: Blonder_Yoav_and_Rivlin_Neria
date: "June 14, 2021"
output: html_document
---




## Submission Instructions 

This lab will be submitted in pairs using GitHub (if you don't have a pair, 
please contact us).  
Please follow the steps in the  [GitHub-Classroom Lab 3](https://classroom.github.com/g/9t2M5lza) to create your group's Lab 3 repository.  
**Important: your team's name must be `FamilyName1_Name1_and_FamilyName2_Name2`**.  
You can collaborate with your partner using the git environment; You can either 
make commits straight to master, or create individual branches (recommended). 
However, once done, be sure to merge your branches to master - you will be 
graded using the most recent *master* version - your last push and merge 
before the deadline.   
**Please do not open/review other peoples' repositories - we will be notified by GitHub if you do.**

Your final push should include this Rmd file (with your answers) together with 
the html file that is outputted automatically by knitr when you knit the Rmd. 
Anything else will be disregarded. In addition, please adhere to the following 
file format:    
`Lab_3_FamilyName1_Name1_and_FamilyName2_Name2.Rmd/html`      


The lab consists of $15$ questions. Each question is worth $7$ points. Questions vary in difficulty and length. <br>
Some questions may require data wrangling and manipulation which you need to 
decide on.   
In some graphs you may need to change the graph limits. 
If you do so, please include the outlier points you have removed in a separate table.  

Show numbers in plots/tables using standard digits and not scientific display. 
That is: 90000000 and not 9e+06.  
Round numbers to at most 3 digits after the dot - that is, 9.456 and not 9.45581451044

The required libraries are listed in the Rmd file below the instructions. You are allowed 
to add additional libraries if you want. <br>
If you do so, *please explain what libraries you've added, and what is each new library used for*. 

##############################################################################


```{r, echo = FALSE, warning=FALSE, message=FALSE, results = 'hide'}
library(ggplot2)
library(tidyverse)
library(tidytext)
library(stringr)
options(scipen=999)
library(wordcloud2)
library(spgs)
library(rvest)

```

I add wordcloud2 to do the wordcloud
tidytext for do alot of thinks in text
<br>


**Background:** The lab will focus on text analysis, sampling and inference. We will extract text from wikipedia describing 
notable female scientists from the 20th century. 


1. Use the library `rvest` to scrape all the **names** of notable female scientists of the 20th century from 
[here](https://en.wikipedia.org/wiki/List_of_female_scientists_in_the_20th_century). For ease of exctraction, you can extract only scientists with known birth or death year. 
You should end up with a `names` vector of at least `488` elements, where each element is a name of a different female scientist. 

```{r}
url <- "https://en.wikipedia.org/wiki/List_of_female_scientists_in_the_20th_century"
html <- read_html(url)
text.all <- html_text(html)
vec_strings<-strsplit(text.all,split = '\n')[[1]]
sic_fem_vec <-vec_strings[47:570]
name_vec <- c(str_extract(sic_fem_vec,".*[0-9]{4}"))
name_vec_1 <- sub(pattern = "born", replacement = "", name_vec)
name_vec_2 <- c(str_extract(name_vec_1,"[^0-9]+"))
name_vec_3 <- c(name_vec_2[!is.na(name_vec_2)])
name_vec_4 <- c(str_extract(name_vec_3,"[^()]+"))
names <- c(str_extract(name_vec_4,"[^,]+"))
```

2. When you click on each scientist name, you are transferred into a different url containing text about this scientist. 
For example, clicking on the first name `Katharine Bartlet`, brings you [here](https://en.wikipedia.org/wiki/Katharine_Bartlett).
Parse the data and create a new vector variable `names_urls` containing the url for each scientist. 
You may need to modify the names to get the exact urls. 
You don't have to be perfect here, and it is enough to get the correct urls for at least $400$ out of the $488$ scientists.   

```{r}
new_names <- c(sub(pattern = " ", replacement = "_", names))
new_names_1 <- c(str_extract(new_names,"[^ ]+"))
stringurl <- c(rep("https://en.wikipedia.org/wiki/",489))
names_urls <- str_c(stringurl,new_names_1)
```


3. Next we would like to retrieve the actual texts about each scientist. 
Write a function called `wiki_text_parser` that given a specific scientist's unparsed html page text as input, 
outputs the parsed biographical text as a string. <br>
The text should start at the line below the line `From Wikipedia, the free encyclopedia` in the wikipedia page. <br>
The text should end right before the `References` of the wikipedia page. See for example the highlighted text below. <br>
Run the function on the first name `Katharine Bartlet` and verify that the biographical text is extracted correctly. <br>
**Hint:** You can look at occurances of the scientist name
<img src="Katharine.png" width="800" height="450" />


```{r}
wiki_text_parser <- function(url){
new_paragraphs <- read_html(url) %>% html_nodes("p") %>% html_text()
x = length(new_paragraphs)
new_vec<-c(strsplit(new_paragraphs,split = '\n'))
(t <- paste(new_vec,collapse=""))
t <- c(gsub(pattern = "\"", replacement = "" , t))
return(t)
}
url_2 = "https://en.wikipedia.org/wiki/Katharine_Bartlett"
wiki_text_parser(url_2)
```


4. Retrieve all the parsed scientists biographies into a vector called `bio`. Use your functions from (a.-c.). <br>
**Note:** reading all biographies may take a few minutes. 
Some errors may occur, but make sure that your pages urls (part b.) match and retrieve 
successfully at least **two thirds** out of the $488$ biographies. <br>
**Hint:** You can use the `try` command to run another command such that if the command fail the program continues and is not stopped. 

```{r}
results  <- lapply(names_urls, function(x) try(wiki_text_parser(x)))
is.error <- function(x) inherits(x, "try-error")
succeeded <- !vapply(results, is.error, logical(1))
bio_1 <- c(results[succeeded])
bio_2 <- c(unlist(bio_1))
bio_3 <- sub(pattern = "c(,)", replacement = "" , bio_2)
index <- (str_which(bio_3,"may refer to:"))
bio <- bio_3[-index]

```

We get 369/489 bio

5. Find the scientist with the **shortest** and with the **longest** biography (in terms of total number of English characters). 


```{r}
shortest_bio <-  bio[[which.min(str_length(bio))]]
longest_bio <-  bio[[which.max(str_length(bio))]]

```
The scientist with the shortest is Bice Sechi-Zorn
The scientist with the longest is Margaret Alice Murray



6. Find the astronomer whose son was a distinguished statistician.  List all the titles of her publications in the jounral `Nature`.


```{r}
find_sic <- str_detect(bio, "astronomer") 
astronomer <- bio[which(str_detect(bio, "astronomer"))]
astronomer_son_statistician <- astronomer[which(str_detect(astronomer, "distinguished"))]
astronomer_son_statistician_1 <- astronomer_son_statistician[which(str_detect(astronomer_son_statistician, "probability"))]
astronomer_son_statistician_1[[2]]
```
Ruby Violet Payne-Scott his son is
Peter Gavin Hall – a mathematician who worked in theoretical statistics and probability theory


7. Retrieve all words appearing in any of the biographies and compute their frequencies. (treat all the texts of the biographies of the scientists as one large document and compute the frequecnies in this document). <br>
Remove all common stop words (use the command `stop_words` from the *tidytext* package), and also all hashtages ('words' starting with '#') and twitter user names ('words' starting with '@'). <br>
Display in a `word-cloud` the top-100 (most-common) remaining words using the computed frequencies. 


```{r}
clear_bio <- gsub("[[:punct:]]", '', bio)
clear_bio <- gsub("[[:digit:]]", '', clear_bio)
word_counts <- as.data.frame(table(unlist( strsplit(clear_bio, "\ ") )))
word_counts <- with(word_counts, word_counts[ Var1 != "", ] )
stopwords <- stop_words
names(word_counts)[1] <- 'word'
word_counts %<>% filter(!grepl("[^A-Za-z]", .data$word) &
                               !(tolower(.data$word) %in% stopwords$word) & 
                               !str_starts(.data$word, "#") &
                               !str_starts(.data$word, '@') &
                               .data$word != '–') %>% arrange(desc(.data$Freq))
top_words <- word_counts %>% head(100)
wordcloud2(data=top_words, size = 0.4, shape = 'pentagon',gridSize=10 )
```

8. Compute the frequency $n_i$ of each of the $26$ letters in the English alphabet. <br>
Consider uppercase and lowercase as the same letter. <br> 
Plot the sorted frequencies after normalization $p_i = n_i / n$ where $n = \sum_{i=1}^{26}$ it the total number of english letters, in a bar-plot. 

```{r}
upper_bio <- toupper(bio)
letter_counts <- as.data.frame(table(unlist( strsplit(upper_bio, "") )))
letter_counts <- with(letter_counts, letter_counts[ Var1 != "", ] )
names(letter_counts)[1] <- 'letter'
ABC = c("A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q",
"R","S","T","U","V","W","X","Y","Z")
vec_name <- names(letter_counts)
m = matrix(0,1,2)
new_data8 <- data.frame(m)
names(new_data8) <- c(vec_name)
for (i in 1:26){
  new_data <- filter(letter_counts,letter == ABC[i])
  new_data8 <- rbind(new_data8,new_data)}
new_data8 <- new_data8[2:27,]
new_data8$Freq <- new_data8$Freq/sum(new_data8$Freq)
new_data8$Freq <- round(new_data8$Freq,4)
new_data8_s <- new_data8 %>% arrange(desc(Freq))
            p<-ggplot(new_data8_s, aes(x=letter, y=Freq, fill=Freq)) +
  geom_bar(stat="identity")
p
```


9. Compute the frequencies of consecutive **pairs** of letters for all $26^2$ ordered pairs of English letters in the biographies text. <br>
That is, create a $26 \times 26$ table where for each two letters $i$ and $j$ the entry $(i,j)$ contains $n_{ij}$, the number of occuracnes of the
two letters appearing consecutively. Count only pairs of letters apearning in the same word. <br>
For example, if the biographies text was: `Angela Merkel` then the count for `el` in your table should be 2, the count for `ng` should be 1, 
and the count for `am` should be 0. <br>
What is the *most common* pair of letters? what is the *least common* pair?  

```{r}
letter_counts_9 <- ((unlist( strsplit(upper_bio, "") )))
pair_letter_counts_9 <- pair.counts(letter_counts_9, case=c("upper"), circular=FALSE)
pair_letter_counts_9d <- data.frame(pair_letter_counts_9)
pair_letter_counts_9A <- pair_letter_counts_9d[c(ABC)]
pair_letter_counts_9B <- pair_letter_counts_9A[c(ABC), ]
most_freq_pair_ind <- which(pair_letter_counts_9B==max(pair_letter_counts_9B), arr.ind=T)
names(pair_letter_counts_9B)[most_freq_pair_ind[1]]
names(pair_letter_counts_9B)[most_freq_pair_ind[2]]
least_freq_pair_ind <- which(pair_letter_counts_9B==min(pair_letter_counts_9B), arr.ind=T)
```

the most freq pair is HE
its 72 pairs of letter 0 times.


10. Let $p_a$ be the actual frequency of the letter `a` in English text, and assume that the true value is the one calculated in 
qu. (8). <br>
For each scientist $s$, consider $\hat{p_a}(s)$ the estimator of this frequency, obtained as the frequency of the letter `a` in the biography text of this scientist only out of all english letters in this text. (That is, we obtain hundreds of estimators for the same parameter) <br>
Compute these estimators and show them in a histogram with $50$ bins. Show also the true value $p_a$ on the plot. <br> 
Plot the absolute error $|\hat{p_a}(s)-p_a|$ vs. the length of the text for each scientist denoted $l(s)$. What is your conclusion? <br>
Compute and plot on the same figure the theoretical standard deviation of $\hat{p_a}(s)$ assuming it is based on a random text with $l(s)$ i.i.d. letters, each has a probability of $p_a$ to be `"a"`. 
Explain your results. <br> 
**Note**: The actual standard deviation is a complicated function of the text length for English text since letters written in the text are not independent. Nevertheless, the i.i.d. model gives a useful guideline. 


```{r}
find_a <- function(bio_2){
  upper_bio <- toupper(bio_2)
a_counts <- as.data.frame(table(unlist( strsplit(upper_bio, "") )))
a_counts <- with(a_counts, a_counts[ Var1 != "", ] )
names(a_counts)[1] <- 'a'
ABC = c("A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q",
"R","S","T","U","V","W","X","Y","Z")
vec_name <- names(a_counts)
m = matrix(0,1,2)
new_data10 <- data.frame(m)
names(new_data10)[1:2] <- c(vec_name)
for (i in 1:26){
  new_data10a <- filter(a_counts,a == ABC[i])
  new_data10 <- rbind(new_data10,new_data10a)}
new_data10$leght_bio <- sum(new_data10$Freq)
new_data10$Freq <- new_data10$Freq/sum(new_data10$Freq)
new_data10$Freq <- round(new_data10$Freq,4)
new_data10 <- new_data10[2,]
return(new_data10)}
d = matrix(0,1,3)
p_hat_ta <- data.frame(d)
names(p_hat_ta) <- c("a","Freq","leght_bio")
for (p in 1:360){
p_hat_ta <- rbind (p_hat_ta,find_a(bio[p]))  
}
p_hat_ta <- p_hat_ta[2:361,]
p_10 <- ggplot(data=p_hat_ta, aes(Freq)) + 
  geom_histogram(breaks=seq(0.05, 0.125, by=0.0015),fill="green")+geom_vline(aes(xintercept = 0.0852), colour="red")+ggtitle("freq of p_hat of A")
p_10
p_hat_ta$absolute_error <- abs(0.0852-p_hat_ta$Freq)
p_10_1 <- ggplot(data = p_hat_ta, aes(x = absolute_error, y = leght_bio))
p_10_1 + geom_point(data = p_hat_ta, color = "blue") +ggtitle("freq of dif of P and P hat")
p_hat_ta$piid <- 0
p_hat_ta$sd <- 0

for (s in 1:360){
poll = sample(ABC, p_hat_ta$leght_bio[s], prob= new_data8_s$Freq, replace = TRUE)
p_hat_ta$piid[s] <-  round((length (poll[which(poll=="A")]))/(p_hat_ta$leght_bio[s]),4)
p_hat_ta$sd[s] <- round((sqrt((p_hat_ta$piid[s])*(1-p_hat_ta$piid[s]))/(sqrt(p_hat_ta$leght_bio[s]))),4)
}
p_10_B <- ggplot(data=p_hat_ta, aes(sd)) + 
  geom_histogram(breaks=seq(0, 0.018, by=0.00036),fill="orange")+ggtitle("p hat iid sd")
p_10_B


```
its look like a short bio the error biger.
in the iid sample we can see almost all the sd very small. we can explain the bigger with a short bio. its look like if the length was des normal the sd will be normal to.

11. Simulate $10,000$ words of length $4$ as follows: <br>
Sample the four characters of the word as i.i.d. random varaiables from the categorical distribution with the values being the $26$ English letters and the probabilities being the $26$ letter frequencies $p_i$ computed in qu. (8). <br>
Next, read the list of English words in the dictionary from the file [words_alpha](https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt). <br>
What fraction of the words you have simulated are actual English words according to this dictrionary?


```{r}
vec_4_words = rep(0,10000)
for(k in 1:10000){
vec_4_words[k] = paste(sample(size = 4, x =ABC,prob= new_data8$Freq,replace = TRUE) , collapse="")}
English_words_dictionary <- data.frame(read.table(url("https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt")))
names(English_words_dictionary) <- "word"
df_word4_l <- data.frame(tolower(vec_4_words))
names(df_word4_l) <- "word"
word_in_dict <-  merge(x = df_word4_l, y = English_words_dictionary, by = "word")
prob_11 <- length(word_in_dict$word)
```

we have 669/10000 =6.69% random word in the dict.

12. Repeat the above question, but this time simulate words as follows:  <br>
Define $p_{ij}$ to be the conditional probability of letter $j$ to appear after letter $i$, calculated as $p_{ij} = n_{ij} / \sum_{k=1}^{26} n_{ik}$, where the $n_{ij}$'s are computed in qu. (9). <br>
Let the characters of the words be $c_1,c_2,c_3,c_4$. $c_1$ is simulated as before. <br>
Next, $c_2$ is simulated *conditional* on $c_1$ with probability: $Prob(c_2 = j | c_1 = i) = p_{ij}$. <br>
Similarly,  $c_3$ is simulated *conditional* on $c_1$ with probability: $Prob(c_3 = j | c_2 = i) = p_{ij}$ <br>
and finally  $c_4$ is simulated *conditional* on $c_1$ with probability: $Prob(c_4 = j | c_3 = i) = p_{ij}$. <br>
What fraction of simulated words match actual English words? Which of the two random models (from this and the previous question) is more similar to true English text? why do you think this is the case? 

```{r}
df12 <- data.frame(matrix(0,1,3))
names(df12) <- c("letter1","letter2","Freq")
for (g in 1:26){
  for (t in 1:26){
  new_data12a <- c(
    tolower(names(pair_letter_counts_9B)[g]), tolower(names(pair_letter_counts_9B)[t]), pair_letter_counts_9B[g,t]/sum(pair_letter_counts_9B[g,]))
  df12 <- rbind(df12,new_data12a)  
  }}
df12$Freq <- as.numeric(df12$Freq)
df12$Freq <- round(df12$Freq,4)
df_pair <- df12[2:677, ]
one_letter_p <- select(new_data8,letter,Freq)
one_letter_p$letter = tolower(one_letter_p$letter)
vec_4_words_2 = rep(0,40000)
for(k in 1:40000){
    if (k==1%%4){
  vec_4_words_2[k] = paste(sample(size = 1, x =one_letter_p$letter,prob= one_letter_p$Freq,replace = TRUE)) }
    else{
      filterdf12 <- filter(df_pair,letter1 == vec_4_words_2[k-1])
      vec_4_words_2[k]= paste(sample(size = 1, x =filterdf12$letter2, prob= filterdf12$Freq,replace = TRUE))
    }}
word_4_letter_vec <- rep(0,10000)
for (x in 1:10000){
  word_4_letter_vec[x] <- paste0(vec_4_words_2[(4*(x-1))+1],vec_4_words_2[(4*(x-1))+2],vec_4_words_2[(4*(x-1))+3],vec_4_words_2[(4*(x-1))+4])}
df_word4_l_p <- data.frame((word_4_letter_vec))
names(df_word4_l_p) <- "word"
word_in_dict_p <-  merge(x = df_word4_l_p, y = English_words_dictionary, by = "word")
prob_12 <- length(word_in_dict_p$word)

```

we have 2178/10000 =21.78% random word in the dict.
its triple then last Q

13. Suppose that a Monkey types a sentence with **two** words by typing randomly one of the words from the biographies, and then typing another one independently *with replacement*, and where the probability of each word is proportional to its frequency in the biographies calculated in qu. (7). <br> 
What is the probability that the same word will be typed twice? Answer using a Monte-carlo simulation with at least $10000$ random word-pairs. Can you calculate the probability exactly?


```{r}
vec_2_words = rep(0,40000)
for(k in 1:40000){
  vec_2_words[k] = paste(sample(size = 1, x =word_counts$word,prob= word_counts$Freq,replace = TRUE))}
vec_2_words_1 = vec_2_words[seq(1,39999 ,by=2)]
vec_2_words_2 = vec_2_words[seq(2,40000 ,by=2)]
df_13 <- data.frame(vec_2_words_1,vec_2_words_2)
names(df_13)= c("word1","word2")
filter_same_word = filter(df_13,word1==word2)
prob_13 <- length(filter_same_word$word1)

```

I got 15/20000 this is mean the prob to get same word twice is 0.075%
its not the exactly prob but the sample is big and is close to the prob.

14. Suppose that a Monkey types a sentence with eight words by typing randomly one of the *words*: `Maria`, `Skłodowska`, `Curie`,  `Pierre`,  `Curie`, `Irène`, `Joliot` and `Curie`  *without replacement* (the word `Curie` appears three times). <br>
What is the probability that the Monkey will type a sentence with the scientists names `Maria Skłodowska Curie`, `Pierre Curie` and `Irène Joliot Curie` appearing in it? (the three scientists names can appear in any order, but the name of each scientist should appear exactly as listed here, i.e. family name appearning last)? <br>
Perform a Monte-carlo simulation with at least $50000$ random sentences to estimate the probability. Can you calculate the probability exactly? When answering, please ignore spaces between words.

I change the L in Skłodowska to l and the E in Irène to E to do it more clear its not matter for the Q
```{r}
word_sen = c("Maria", "Sklodowska", "Curie",  "Pierre",  "Curie", "Irene", "Joliot", "Curie")
vec_sen_words = rep(0,400000)
for(k in 1:50000){
  vec_sen_words[((8*(k-1))+1):((8*(k-1))+8)] = paste(sample(size = 8, x =word_sen,replace = FALSE))}

word_8_sen_vec <- rep(0,50000)
for (x in 1:50000){
  word_8_sen_vec[x] <- paste(vec_sen_words[((8*(x-1))+1)],vec_sen_words[((8*(x-1))+2)],vec_sen_words[((8*(x-1))+3)],vec_sen_words[((8*(x-1))+4)],vec_sen_words[((8*(x-1))+5)],vec_sen_words[((8*(x-1))+6)],vec_sen_words[((8*(x-1))+7)],vec_sen_words[((8*(x-1))+8)])}
df_14 <- data.frame(word_8_sen_vec)
names(df_14) <- "sen"
fiter_df_14 <- filter(df_14, sen=="Maria Sklodowska Curie Pierre Curie Irene Joliot Curie"|sen=="Pierre Curie Maria Sklodowska Curie Irene Joliot Curie"|sen=="Irene Joliot Curie Pierre Curie Maria Sklodowska Curie"|sen=="Maria Sklodowska Curie Irene Joliot Curie Pierre Curie"|sen=="Pierre Curie Irene Joliot Curie Maria Sklodowska Curie"|sen=="Irene Joliot Curie Maria Sklodowska Curie Pierre Curie")
prob_14 <- length(fiter_df_14$sen)

```

I got 53 from 50000 the prob is 0.106% to get the 3 names.
its not exectly the prob but because the sample are 50000 its close.

15. What is the probability that a Monkey typing randomly one of the $26$ English *letters* (i.e. using a *uniform distribution*) one after the other (with replacement) will type on her first attempt the name `Rosalind Franklin`? can you estimate it by sampling? explain (ignore spaces and upper/lower case here too).  <br>
What will be the probability if the Monkey types letters with probability proportional to their frequencies as computed from the text in qu. (8)? <br>
**Hint:** It is easier to work with log-probabilities

```{r}
vec_Rosalind_Franklin = rep(0,800000)
abc= tolower(ABC)
for(k in 1:50000){
  vec_Rosalind_Franklin[((16*(k-1))+1):((16*(k-1))+16)] = paste(sample(size = 16, x =abc,replace = TRUE))}

vec_Rosalind_Franklin_1 <- rep(0,50000)
for (x in 1:50000){
  vec_Rosalind_Franklin_1[x] <- paste(paste0(toupper(vec_Rosalind_Franklin[((16*(x-1))+1)]),vec_Rosalind_Franklin[((16*(x-1))+2)],vec_Rosalind_Franklin[((16*(x-1))+3)],vec_Rosalind_Franklin[((16*(x-1))+4)],vec_Rosalind_Franklin[((16*(x-1))+5)],vec_Rosalind_Franklin[((16*(x-1))+6)],vec_Rosalind_Franklin[((16*(x-1))+7)],vec_Rosalind_Franklin[((16*(x-1))+8)]),paste0(toupper(vec_Rosalind_Franklin[((16*(x-1))+9)]),vec_Rosalind_Franklin[((16*(x-1))+10)],vec_Rosalind_Franklin[((16*(x-1))+11)],vec_Rosalind_Franklin[((16*(x-1))+12)],vec_Rosalind_Franklin[((16*(x-1))+13)],vec_Rosalind_Franklin[((16*(x-1))+14)],vec_Rosalind_Franklin[((16*(x-1))+15)],vec_Rosalind_Franklin[((16*(x-1))+16)]))}
df_15 <- data.frame(vec_Rosalind_Franklin_1)
names(df_15) <- "words_Rosalind_Franklin"
filter_Rosalind_Franklin <- filter(df_15,words_Rosalind_Franklin=="Rosalind Franklin")
prob_15a <- length(filter_Rosalind_Franklin$words_Rosalind_Franklin)


new_data15 <- new_data8
new_data15$letter <- tolower(new_data15$letter)
vec_Rosalind_Franklin_p = rep(0,800000)
for(k in 1:50000){
  vec_Rosalind_Franklin_p[((16*(k-1))+1):((16*(k-1))+16)] = paste(sample(size = 16, x =new_data15$letter,prob=new_data15$Freq, replace = TRUE))}

vec_Rosalind_Franklin_p_1 <- rep(0,50000)
for (x in 1:50000){
  vec_Rosalind_Franklin_p_1[x] <- paste(paste0(toupper(vec_Rosalind_Franklin_p[((16*(x-1))+1)]),vec_Rosalind_Franklin_p[((16*(x-1))+2)],vec_Rosalind_Franklin_p[((16*(x-1))+3)],vec_Rosalind_Franklin_p[((16*(x-1))+4)],vec_Rosalind_Franklin_p[((16*(x-1))+5)],vec_Rosalind_Franklin_p[((16*(x-1))+6)],vec_Rosalind_Franklin_p[((16*(x-1))+7)],vec_Rosalind_Franklin_p[((16*(x-1))+8)]),paste0(toupper(vec_Rosalind_Franklin_p[((16*(x-1))+9)]),vec_Rosalind_Franklin_p[((16*(x-1))+10)],vec_Rosalind_Franklin_p[((16*(x-1))+11)],vec_Rosalind_Franklin_p[((16*(x-1))+12)],vec_Rosalind_Franklin_p[((16*(x-1))+13)],vec_Rosalind_Franklin_p[((16*(x-1))+14)],vec_Rosalind_Franklin_p[((16*(x-1))+15)],vec_Rosalind_Franklin_p[((16*(x-1))+16)]))}
df_15_p <- data.frame(vec_Rosalind_Franklin_p_1)
names(df_15_p) <- "words_Rosalind_Franklin"
filter_Rosalind_Franklin_p <- filter(df_15_p,words_Rosalind_Franklin=="Rosalind Franklin")
prob_15b <- length(filter_Rosalind_Franklin_p$words_Rosalind_Franklin)

```
in the to way I dont get in 50000 samples any of Rosalind Franklin
because the chance to get this is 1/26^16 and this a very big number.